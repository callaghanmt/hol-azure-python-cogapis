{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can use free [Python notebook service](http://notebooks.azure.com) to run the below code. Because of security reasons, these kind of cloud services cant access any file on Internet. One of the exception the above service is that if the file is on Azure Storage Service, you can access them without problem.\n",
    "\n",
    "You can also run the below samples in your loca Jupyter notebook server without any limitation etc.\n",
    "\n",
    "### Azure Storage API\n",
    "Almost all Cognitive Services API require input data. i.e. image data to detect faces. This section will show how to use the Azure Storage API to store data in your own cloud storage service with Python APIs.\n",
    "\n",
    "#### Sign-up Azure Storage Service\n",
    "1. Create an Azure Storage account with following steps:\n",
    "    1. Go to the Azure Portal at http://portal.azure.com/ and sign in with your Azure account.\n",
    "    2. Click the New icon on the top left of the Portal, then click Data + Storage > Storage Account. Click the Create button, then give the storage account a unique name and create a new resource group for it. When the storage account has been created, the Notifications button will flash a green SUCCESS and the storage account's blade is open to show that it belongs to the new resource group you created.\n",
    "    3. Click the Access keys part in the storage account's blade. Take note of the **account name** and **key1**.\n",
    "\n",
    "2. Using open source, cross-platform [\"Storage Explorer\"](http://storageexplorer.com/) tool and the above **account name** and **key1**, create a container under your storage service.\n",
    "\n",
    "3. You can find the sample files under appendix folder of this repository. Upload these or similar own files into your Azure storage account, so they will be used in the python sample codes below.\n",
    "\n",
    "Below steps uses [Microsoft Azure Storage SDK for Python](https://github.com/Azure/azure-storage-python) to access Azure Storage Service.\n",
    "\n",
    "\n",
    "## ATTENTION\n",
    "Below storage account (**azure_storage_account_name = 'cogsdemosstorage'**) doesnt exist anymore! You should create your own storage account. If you dont want to access your storage account with a KEY (as in the below sample where key value is null) you should set your storage account/container as public access. After creating a storage account and a public container inside it, please copy few JPEG, PNG, WAV files that you may found free ones with BING/GOOGLEsearch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "azure_storage_account_name = 'cogsdemosstorage'\n",
    "azure_storage_account_key = None  # dont need key... we will access public blob... \n",
    "\n",
    "if azure_storage_account_name is None:\n",
    "    raise Exception(\"You must provide a name for an Azure Storage account\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATTENTION\n",
    "Below line installs AZURE STORAGE ACCOUNT LIBRARIES required to access Azure Blob Storage Service. To run the below code/cell, you MUST launch the Jupyter notebook in ADMINISTRATOR mode... i.e.: Open command prompt with admin previliges and run \"Jupyter notebook\" command inside that cmd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-storage==0.32.0\n",
      "  Using cached azure_storage-0.32.0-py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil in c:\\program files\\anaconda3\\lib\\site-packages (from azure-storage==0.32.0)\n",
      "Requirement already satisfied: requests in c:\\program files\\anaconda3\\lib\\site-packages (from azure-storage==0.32.0)\n",
      "Collecting azure-common (from azure-storage==0.32.0)\n",
      "  Using cached azure_common-1.1.8-py2.py3-none-any.whl\n",
      "Collecting azure-nspkg (from azure-storage==0.32.0)\n",
      "  Using cached azure_nspkg-2.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in c:\\program files\\anaconda3\\lib\\site-packages (from python-dateutil->azure-storage==0.32.0)\n",
      "Installing collected packages: azure-nspkg, azure-common, azure-storage\n",
      "Successfully installed azure-common-1.1.8 azure-nspkg-2.0.0 azure-storage-0.32.0\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-storage==0.32.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlockBlobService\n",
    "\n",
    "# create blob service object to access the files in the storage\n",
    "# You can access your account_name and account_key values at [Azure Management Portal](https://portal.azure.com)\n",
    "blob_service = BlockBlobService(azure_storage_account_name, azure_storage_account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "List sample data files in a specific container (folder), so we can select one of them to work on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# select container (folder) name where the files resides\n",
    "container_name = 'sampledata'\n",
    "\n",
    "# list files in the selected folder\n",
    "generator = blob_service.list_blobs(container_name)\n",
    "\n",
    "blob_prefix = 'https://{0}.blob.core.windows.net/{1}/{2}'\n",
    "\n",
    "print(\"List of files in the container:\")\n",
    "for blob in generator:\n",
    "    print(blob_prefix.format(blob_service.account_name, container_name, blob.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Read one of the image file from the blob storage as byte array. If you want update the below blob_name variable with one of the file available in the above file list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "blob_name = 'emotion1.jpeg'\n",
    "blob = blob_service.get_blob_to_bytes(container_name, blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using PIL and matplotlib libraries, display the image file content to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "image_file_in_mem = io.BytesIO(blob)\n",
    "img_bytes = Image.open(image_file_in_mem)\n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SEARCH - Bing Image Search API\n",
    "You can use Bing Image Search API to keyword based image search, get insights about an image, find trending images.  \n",
    "Ref: https://msdn.microsoft.com/en-us/library/dn760791.aspx  \n",
    "     https://www.microsoft.com/cognitive-services/en-us/bing-image-search-api\n",
    "\n",
    "Call Bing Image Search API to get images with human faces (in later sections, will be used for face detection etc.) Change search parameters as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests     # used to make rest calls\n",
    "import json # parse response result\n",
    "import urllib\n",
    "\n",
    "url_search_api = 'https://api.cognitive.microsoft.com/bing/v5.0/images/search?' # service address \n",
    "api_key = 'c8042821455b4dc095cceaa0cd0dc67e'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'q': 'facial emotion',\n",
    "    'count': '5',\n",
    "    'offset': '0',\n",
    "    'mkt': 'en-us',\n",
    "    'safeSearch': 'Moderate',\n",
    "    #'imageContent' : 'Portrait', \n",
    "    'imageType' : 'Photo' # AnimatedGif, Clipart, Line, Photo, Shopping\n",
    "})\n",
    "\n",
    "url = url_search_api + params\n",
    "\n",
    "api_response = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print out the call response (which is in Json format) in pretty form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Display search resulting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as ipImage, display\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "for j in res_json['value']:\n",
    "    c_url = j['contentUrl']\n",
    "    pr = urlparse(c_url)\n",
    "    ru = parse_qs(pr.query)\n",
    "    img_url = ru['r'][0]\n",
    "    img = ipImage(url=img_url, width=150, height=200)\n",
    "    \n",
    "    display(img)\n",
    "    print(img_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VISION - Face API\n",
    "You can use Face API to Detect, Find Similar, Group, Identify, Verify faces.  \n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/face-api\n",
    "\n",
    "Call Cognitive Services Face API to detect facial features in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load image file to process\n",
    "blob_name = 'face4.jpeg'\n",
    "blob = blob_service.get_blob_to_bytes(container_name, blob_name)\n",
    "image_file_in_mem = io.BytesIO(blob)\n",
    "img_bytes = Image.open(image_file_in_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "\n",
    "url_face_api = 'https://api.projectoxford.ai/face/v1.0/detect' # service address \n",
    "api_key = 'd469dbc2ba424afd86c0ba0716a62ca8'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Type': 'application/octet-stream', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'returnFaceId': 'true',\n",
    "    'returnFaceLandmarks': 'true',\n",
    "    'returnFaceAttributes': 'age,gender,smile,facialHair,headPose,glasses',\n",
    "})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = url_face_api + query_string\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print out the call response (which is in Json format) in pretty form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get detected face coordiates (bounding squares) from call response and draw red square borders for females, blue for males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Draw face rectangles\n",
    "for i in res_json:\n",
    "    fr = i['faceRectangle'] # get faceRectangle node per detected face in the image\n",
    "\n",
    "    pc = 'red' # patch color\n",
    "    if i['faceAttributes']['gender'] == 'male':\n",
    "        pc = 'blue'\n",
    "\n",
    "    ax.add_patch(\n",
    "        patches.Rectangle(\n",
    "            (fr['left'], fr['top']), fr['width'], fr['height'],\n",
    "            fill=False, linewidth=4, color=pc)\n",
    "    )\n",
    "    \n",
    "    ax.text(fr['left'], fr['top']+fr['height'], \n",
    "            'age:'+str(i['faceAttributes']['age']), \n",
    "            fontsize=14, weight='bold', color='red', bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "\n",
    "ps = 5       # patch size\n",
    "pc = '#00FF00'   # patch color\n",
    "\n",
    "# Draw eye, nose, mouth\n",
    "for i in res_json:\n",
    "    fl = i['faceLandmarks']\n",
    "\n",
    "    # left eye\n",
    "    ax.add_patch(patches.Circle((fl['pupilLeft']['x'], fl['pupilLeft']['y']), ps, color=pc))\n",
    "\n",
    "    # right eye\n",
    "    ax.add_patch(patches.Circle((fl['pupilRight']['x'], fl['pupilRight']['y']), ps, color=pc))\n",
    "\n",
    "    # mouth\n",
    "    ax.add_patch(patches.Circle((fl['mouthLeft']['x'], fl['mouthLeft']['y']), ps, color=pc))\n",
    "    ax.add_patch(patches.Circle((fl['mouthRight']['x'], fl['mouthRight']['y']), ps, color=pc))\n",
    "\n",
    "    # nose\n",
    "    ax.add_patch(patches.Circle((fl['noseTip']['x'], fl['noseTip']['y']), ps, color=pc))\n",
    "\n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VISION - Emotion API\n",
    "You can use Emotion API to Detect, emotion recognition in image, in video.  \n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/emotion-api\n",
    "\n",
    "Below sample shows how to call Cognitive Services Emotion API to detect facial emotion in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load image file to process\n",
    "blob_name = 'emotion1.jpeg'\n",
    "blob = blob_service.get_blob_to_bytes(container_name, blob_name)\n",
    "image_file_in_mem = io.BytesIO(blob)\n",
    "img_bytes = Image.open(image_file_in_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "\n",
    "url_emotion_api = 'https://api.projectoxford.ai/emotion/v1.0/recognize' # service address \n",
    "api_key = '8eae1ba129df43a289015fb45554e074'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Type': 'application/octet-stream', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "api_response = requests.post(url_emotion_api, headers=headers, data=blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print out the call response (which is in Json format) in pretty form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Get detected face coordiates (bounding squares) from call response and draw borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax = plt.gca()\n",
    "\n",
    "# Draw face rectangles\n",
    "for i in res_json:\n",
    "    pc = '#00FF00' # patch color\n",
    "    \n",
    "    fr = i['faceRectangle'] # get faceRectangle node per detected face in the image\n",
    "    ax.add_patch(\n",
    "        patches.Rectangle(\n",
    "            (fr['left'], fr['top']), fr['width'], fr['height'],\n",
    "            fill=False, linewidth=4, color=pc)\n",
    "    )\n",
    "    \n",
    "    s = i['scores']\n",
    "    \n",
    "    frm_txt = '{0: <10}: {1:>.6f}\\n{2: <10}: {3:>.6f}\\n{4: <10}: {5:>.6f}\\n{6: <10}: {7:>.6f}\\n\\\n",
    "{8: <10}: {9:>.6f}\\n{10: <10}: {11:>.6f}\\n{12: <10}: {13:>.6f}\\n{14: <10}: {15:>.6f}'\n",
    "    \n",
    "    lbls =     ['anger', s['anger'],\\\n",
    "                'contempt', s['contempt'],\\\n",
    "                'disgust', s['disgust'],\\\n",
    "                'fear', s['fear'],\\\n",
    "                'happiness', s['happiness'],\\\n",
    "                'neutral', s['neutral'],\\\n",
    "                'sadness', s['sadness'],\\\n",
    "                'surprise', s['surprise']]\n",
    "    \n",
    "    ax.text(fr['left'], fr['top'], frm_txt.format(*lbls), \n",
    "            fontsize=14, weight='bold', color='white', family='monospace',\n",
    "            bbox=dict(facecolor='green', alpha=0.6))    \n",
    "\n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VISION - Computer Vision API - Analyze\n",
    "You can use Computer Vision API to analyze visual content in different ways based on inputs and user choices.  \n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/Computer-Vision-API/documentation \n",
    "\n",
    "Below sample shows how to call Cognitive Services Computer Vision API to detect image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image as ipImage, display\n",
    "img_url = 'https://portalstoragewuprod.azureedge.net/vision/Analysis/7-1.jpg'\n",
    "#img_url = 'http://i.ebayimg.com/00/s/NTUxWDg3MQ==/z/IawAAOSw7NNT6NZY/$_32.JPG'\n",
    "    \n",
    "img = ipImage(url=img_url, width=250, height=250)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "\n",
    "url_vision_api = 'https://api.projectoxford.ai/vision/v1.0/analyze' # service address \n",
    "api_key = '3ae10be9a45442f5910434e1af6c52dd'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'visualFeatures': 'Categories,Tags,Description,Faces,ImageType,Color,Adult',\n",
    "})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = url_vision_api + query_string\n",
    "\n",
    "body = '{\\'url\\':\\'' + img_url + '\\'}'\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print out the call response (which is in Json format) in pretty form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### VISION - Computer Vision API - OCR\n",
    "You can use Computer Vision API to read text in images.  \n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/Computer-Vision-API/documentation\n",
    "Below sample shows how to call Cognitive Services Computer Vision API to detect text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# load image file to process\n",
    "blob_name = 'ocr1.jpeg'\n",
    "blob = blob_service.get_blob_to_bytes(container_name, blob_name)\n",
    "image_file_in_mem = io.BytesIO(blob)\n",
    "img_bytes = Image.open(image_file_in_mem)\n",
    "\n",
    "# Show the original image that we will process\n",
    "img_url = 'https://mkmsstrg4ml.blob.core.windows.net/sampledata/ocr1.jpeg'\n",
    "img = ipImage(url=img_url, width=250, height=250)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "\n",
    "url_vision_api = 'https://api.projectoxford.ai/vision/v1.0/ocr' # service address \n",
    "api_key = '3ae10be9a45442f5910434e1af6c52dd'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Type': 'application/octet-stream', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'language': 'unk',\n",
    "    'detectOrientation': 'true',\n",
    "})\n",
    "\n",
    "query_string = '?{0}'.format(params)\n",
    "\n",
    "url = url_vision_api + query_string\n",
    "\n",
    "api_response = requests.post(url, headers=headers, data=blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Print out the call response (which is in Json format) in pretty form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "ax = plt.gca()\n",
    "\n",
    "text_angle = 0\n",
    "try:\n",
    "    text_angle = res_json['textAngle']\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Draw OCR rectangles\n",
    "for i in res_json['regions']:\n",
    "    pl = i['lines']\n",
    "    for k in pl:\n",
    "        words = k['words']\n",
    "        for l in words:\n",
    "            bb = l['boundingBox']\n",
    "            txt = l['text']\n",
    "            \n",
    "            bb = list(map(int, bb.split(',')))\n",
    "            \n",
    "            ax.add_patch(\n",
    "                patches.Rectangle(\n",
    "                    (bb[0], bb[1]), bb[2], bb[3], angle=text_angle,\n",
    "                    fill=False, linewidth=4, color='#00FF00')\n",
    "            )\n",
    "            \n",
    "            ax.text(bb[0], bb[1], txt, \n",
    "                    fontsize=14, weight='bold', color='red', bbox=dict(facecolor='white', alpha=0.8))\n",
    "            \n",
    "plt.imshow(img_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SPEECH - BING Speech API - TTS\n",
    "\n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/speech-api/documentation/overview \n",
    "\n",
    "The Speech Recognition API provides the ability to convert text to spoken audio by sending text to Microsoftâs servers in the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get access token to use the speech services\n",
    "url_token_api = 'https://api.cognitive.microsoft.com/sts/v1.0/issueToken' # service address \n",
    "api_key = '39f61de6d28e4667bbfda88dd37b12ce'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Length': '0', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "api_response = requests.post(url_token_api, headers=headers)\n",
    "\n",
    "access_token = str(api_response.content.decode('utf-8'))\n",
    "\n",
    "\n",
    "text = '<speak version=\\'1.0\\' xml:lang=\\'en-US\\'>\\\n",
    "            <voice xml:lang=\\'en-US\\' xml:gender=\\'Female\\' \\\n",
    "                        name=\\'Microsoft Server Speech Text to Speech Voice (en-US, ZiraRUS)\\'>\\\n",
    "                The Text-To-Speech API enables you to build smart apps that can speak. You can test it now, \\\n",
    "                simply choose your target language, add your sentences then click on the play button to \\\n",
    "                see how speech synthesis works. \\\n",
    "            </voice>\\\n",
    "        </speak>'\n",
    "\n",
    "\n",
    "# Call Speech to text service\n",
    "url_tts_api = 'https://speech.platform.bing.com/synthesize' # service address \n",
    "\n",
    "headers = {'Authorization': 'Bearer {0}'.format(access_token), \\\n",
    "           'Content-Length': len(text), \\\n",
    "           'Content-type': 'text/plain; charset=utf-8',\\\n",
    "           'X-Microsoft-OutputFormat': 'riff-16khz-16bit-mono-pcm'}\n",
    "\n",
    "api_response = requests.post(url_tts_api, headers=headers, data=text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Play the audio content returned by the service call above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "audio_bytes = Audio(data=api_response.content)\n",
    "display(audio_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SPEECH - BING Speech API - STT\n",
    "\n",
    "Ref: https://www.microsoft.com/cognitive-services/en-us/speech-api/documentation/overview \n",
    "\n",
    "The Speech Recognition API provides the ability to convert spoken audio to text by sending audio to Microsoftâs servers in the cloud. Developers have a choice of using the REST API or the Client Library (for real-time streaming)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# load speech file to process\n",
    "blob_name = 'speech3.wav'\n",
    "blob = blob_service.get_blob_to_bytes(container_name, blob_name)\n",
    "\n",
    "wav_bytes = Audio(data=blob)\n",
    "display(wav_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import uuid\n",
    "\n",
    "# Get access token to use the speech services\n",
    "url_token_api = 'https://api.cognitive.microsoft.com/sts/v1.0/issueToken' # service address \n",
    "api_key = '39f61de6d28e4667bbfda88dd37b12ce'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Length': '0', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "api_response = requests.post(url_token_api, headers=headers)\n",
    "\n",
    "access_token = str(api_response.content.decode('utf-8'))\n",
    "\n",
    "\n",
    "# Call Speech to text service\n",
    "url_stt_api = 'https://speech.platform.bing.com/recognize' # service address \n",
    "\n",
    "headers = {'Authorization': 'Bearer {0}'.format(access_token), \\\n",
    "           'Content-Length': len(blob), \\\n",
    "           'Content-type': 'audio/wav', \\\n",
    "           'codec': 'audio/pcm', \\\n",
    "           'samplerate': '16000'}\n",
    "\n",
    "params = urllib.parse.urlencode({\n",
    "    'scenarios': 'ulm',\n",
    "    'appid': 'D4D52672-91D7-4C74-8AD8-42B1D98141A5', # dont change, it is fixed by design\n",
    "    'locale': 'en-US', # speech in english\n",
    "    'device.os': 'PC',\n",
    "    'version': '3.0',\n",
    "    'format': 'json', # return value in json\n",
    "    'instanceid': str(uuid.uuid1()), # any guid\n",
    "    'requestid': str(uuid.uuid1()),\n",
    "})\n",
    "\n",
    "api_response = requests.post(url_stt_api, headers=headers, params=params, data=blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json = json.loads(api_response.content.decode('utf-8'))\n",
    "\n",
    "print(json.dumps(res_json, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LANGUAGE - BING Text Analytics API - Sentiment, Key Phrases, Languages\n",
    "Extracts information from your text.\n",
    "ref: https://azure.microsoft.com/en-us/documentation/articles/cognitive-services-text-analytics-quick-start/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get access token to use the speech services\n",
    "url_sentiment_api = 'https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/sentiment' # service address \n",
    "url_keyphrases_api = 'https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/keyPhrases' # service address \n",
    "url_languages_api = 'https://westus.api.cognitive.microsoft.com/text/analytics/v2.0/languages' # service address \n",
    "api_key = 'c97786c34cc24041aedfa2a1c64d9aee'          # Azure Cognitive API Key, replace with your own key\n",
    "\n",
    "headers = {'Content-Length': '0', 'Ocp-Apim-Subscription-Key':api_key}\n",
    "\n",
    "text = '{                                           \\\n",
    "            \"documents\": [                          \\\n",
    "                {                                   \\\n",
    "                    \"language\": \"en\",               \\\n",
    "                    \"id\": \"1\",                      \\\n",
    "                    \"text\": \"remember where we were eight years ago we had the worst financial crisis \\\n",
    "                    the great recession the worst since the nineteen thirties\"        \\\n",
    "                },                                  \\\n",
    "            ]                                       \\\n",
    "        }'                                          \\\n",
    "\n",
    "\n",
    "headers = {'Ocp-Apim-Subscription-Key':api_key, \\\n",
    "           'Content-type': 'application/json',\\\n",
    "           'Accept': 'application/json'}\n",
    "\n",
    "api_response_sentiment = requests.post(url_sentiment_api, headers=headers, data=text)\n",
    "api_response_keyphrases = requests.post(url_keyphrases_api, headers=headers, data=text)\n",
    "api_response_languages = requests.post(url_languages_api, headers=headers, data=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "res_json_sentiment = json.loads(api_response_sentiment.content.decode('utf-8'))\n",
    "res_json_keyphrases = json.loads(api_response_keyphrases.content.decode('utf-8'))\n",
    "res_json_languages = json.loads(api_response_languages.content.decode('utf-8'))\n",
    "\n",
    "print('// Sentiment analysis result:')\n",
    "print(json.dumps(res_json_sentiment, indent=2, sort_keys=True))\n",
    "print('\\n\\n// Key phrase analysis result:')\n",
    "print(json.dumps(res_json_keyphrases, indent=2, sort_keys=True))\n",
    "print('\\n\\n//Language analysis result:')\n",
    "print(json.dumps(res_json_languages, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
